{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the SparkContext\n",
    "\n",
    "* `SparkContext` (aka Spark context) is the heart of a Spark application\n",
    "* You could also assume that a SparkContext instance is a Spark application\n",
    "* Spark context sets up internal services and establishes a connection to a Spark execution environment\n",
    "* Once a SparkContext is created you can use it to create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext is stopped)\n",
    "* A Spark context is essentially a client of Sparkâ€™s execution environment and acts as the master of your Spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1067174e0>\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "print(sc)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a SparkSession\n",
    "\n",
    "* What if we're not sure a Spark Session exists?\n",
    "* Using the following method returns an existing Spark Session if there's one already in the environment, else it creates a new one if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x105d765c0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark (a spark session, creates a new one or retrieves current)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Catalog of Tables\n",
    "\n",
    "* A list of all the data inside the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will be empty for now\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting data into a Spark Cluster\n",
    "\n",
    "* `.createDataFrame()` method takes a dataframe and returns a Spark dataframe\n",
    "* The output of this method is stored locally (not in the SparkSession catalog); this means we can use all the Spark DataFrame methods on it, but cannot access the data in other contexts\n",
    "* To use the `.sql()` method, we'd have to save the data as a temporary table\n",
    "* We can do this using the `.createTempView()` Spark DataFrame method\n",
    "* This method registers the dataframe as a table in the catalog, but as this table is temporary, it can only be accessed from the specific `SparkSession` used to create the Spark DataFrame\n",
    "* There is also the `.createOrReplaceTempView()` method which safely creates a new temp table if nothing was there before, or updates an existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the middle man\n",
    "\n",
    "* Why deal with pandas at all? Can we directly read in data? Yes\n",
    "* `SparkSession` has a `.read` attribute which has several methods for reading different data sources into Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|CARRIER|FL_NUM|ORIGIN|DEST|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "|2015|    1|           1|          4|     AA|     1|   JFK| LAX|     855|       -5|    1237|        7|        0|             null|     378|    2475|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"airline_data/train_df.csv\"\n",
    "\n",
    "airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "airports.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='airports', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Add spark_temp to the catalog\n",
    "airports.createOrReplaceTempView(\"airports\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# OR maybe this?\n",
    "\n",
    "# from pyspark.sql import HiveContext\n",
    "# sqlContext = HiveContext(sc)\n",
    "\n",
    "# flights.write.format(\"orc\").saveAsTable(\"flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrames & SQL\n",
    "\n",
    "* Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster.\n",
    "\n",
    "* RDDs are hard to work with directly; we'll use the Spark DataFrame abstraction built on top of RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|CARRIER|FL_NUM|ORIGIN|DEST|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "|2015|    1|           1|          4|     AA|     1|   JFK| LAX|     855|       -5|    1237|        7|        0|             null|     378|    2475|\n",
      "|2015|    1|           2|          5|     AA|     1|   JFK| LAX|     850|      -10|    1211|      -19|        0|             null|     357|    2475|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"FROM airports SELECT * LIMIT 2\"\n",
    "\n",
    "airports10 = spark.sql(query)\n",
    "\n",
    "airports10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORD</td>\n",
       "      <td>PDX</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNA</td>\n",
       "      <td>PHX</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL</td>\n",
       "      <td>GSP</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EWR</td>\n",
       "      <td>STT</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CLE</td>\n",
       "      <td>SJU</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin dest    N\n",
       "0    ORD  PDX  786\n",
       "1    SNA  PHX  276\n",
       "2    ATL  GSP  914\n",
       "3    EWR  STT  181\n",
       "4    CLE  SJU   30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        SELECT origin, dest, COUNT(*) AS N FROM airports \n",
    "        GROUP BY origin, dest\n",
    "        \"\"\"\n",
    "\n",
    "flight_counts = spark.sql(query)\n",
    "pd_counts = flight_counts.toPandas()\n",
    "pd_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Manipulating Data\n",
    "\n",
    "## Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|CARRIER|FL_NUM|ORIGIN|DEST|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|duration_hrs|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|2015|    1|           1|          4|     AA|     1|   JFK| LAX|     855|       -5|    1237|        7|        0|             null|     378|    2475|         6.3|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column which shows the duration in hours for each flight\n",
    "flights = spark.table(\"airports\")\n",
    "flights = flights.withColumn('duration_hrs', flights.AIR_TIME/60)\n",
    "flights.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "* The `.filter()` method is Spark's counterpart of SQL's `WHERE` clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|CARRIER|FL_NUM|ORIGIN|DEST|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|duration_hrs|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|2015|    1|           1|          4|     AA|     1|   JFK| LAX|     855|       -5|    1237|        7|        0|             null|     378|    2475|         6.3|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|YEAR|MONTH|DAY_OF_MONTH|DAY_OF_WEEK|CARRIER|FL_NUM|ORIGIN|DEST|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|AIR_TIME|DISTANCE|duration_hrs|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "|2015|    1|           1|          4|     AA|     1|   JFK| LAX|     855|       -5|    1237|        7|        0|             null|     378|    2475|         6.3|\n",
      "+----+-----+------------+-----------+-------+------+------+----+--------+---------+--------+---------+---------+-----------------+--------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_flights1 = flights.filter(\"DISTANCE > 1000\")\n",
    "long_flights2 = flights.filter(flights.DISTANCE > 1000)\n",
    "\n",
    "long_flights1.show(1) == long_flights2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting I\n",
    "\n",
    "* Spark's variant of SQL's `SELECT` is the `.select()` method\n",
    "* The difference between `.select()` and `.withColumn()` is that the former returns only the columns we specifiy, whereas the latter returns all the columns of the DataFrame in addition to the one we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+\n",
      "|ORIGIN|DEST|CARRIER|\n",
      "+------+----+-------+\n",
      "|   SEA| LAX|     DL|\n",
      "|   SEA| LAX|     DL|\n",
      "+------+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected1 = flights.select(\"FL_NUM\", \"ORIGIN\", \"DEST\")\n",
    "temp = flights.select(flights.ORIGIN, flights.DEST, flights.CARRIER)\n",
    "\n",
    "filterA = flights.ORIGIN == 'SEA'\n",
    "filterB = flights.DEST == 'LAX'\n",
    "\n",
    "selected2 = temp.filter(filterA).filter(filterB)\n",
    "selected2.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting II\n",
    "\n",
    "* Similar to SQL, we can use `.select()` to perform column-wise operations\n",
    "* When selecting using the `df.colName` notation, we can perform any column operations and the `.select()` method will return the transformed column\n",
    "* We can also use the `.alias()` method to rename a column we're selecting\n",
    "* The equivalent Spark DataFrame method `.selectExpr()` takes SQL expressions as a string with the `as` keyword being equivalent to the `.alias()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-----------------+\n",
      "|ORIGIN|DEST|FL_NUM|        avg_speed|\n",
      "+------+----+------+-----------------+\n",
      "|   JFK| LAX|     1|392.8571428571429|\n",
      "+------+----+------+-----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+------+----+------+-----------------+\n",
      "|ORIGIN|DEST|FL_NUM|        avg_speed|\n",
      "+------+----+------+-----------------+\n",
      "|   JFK| LAX|     1|392.8571428571429|\n",
      "+------+----+------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_speed = (flights.DISTANCE/(flights.AIR_TIME/60)).alias(\"avg_speed\")\n",
    "speed1 = flights.select(\"ORIGIN\", \"DEST\", \"FL_NUM\", avg_speed)\n",
    "\n",
    "speed2 = flights.selectExpr(\"ORIGIN\", \"DEST\", \"FL_NUM\",\n",
    "                            \"DISTANCE/(AIR_TIME/60) as avg_speed\")\n",
    "speed1.show(1) == speed2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating I\n",
    "\n",
    "* The common aggregation methods `.min()`, `.max()`, and `.count()` are `GroupedData` methods, and are created by calling the `.groupBy()` DataFrame method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AIR_TIME: float]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(flights.AIR_TIME.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_num = flights.select(flights.DISTANCE.cast(\"float\"), \n",
    "                             flights.ORIGIN, \n",
    "                             flights.CARRIER,\n",
    "                             flights.AIR_TIME.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|min(DISTANCE)|\n",
      "+-------------+\n",
      "|        236.0|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|max(AIR_TIME)|\n",
      "+-------------+\n",
      "|        412.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_num.filter(flights.ORIGIN=='LAX').groupBy().min(\"DISTANCE\").show()\n",
    "flights_num.filter(flights.ORIGIN=='SEA').groupBy().max(\"AIR_TIME\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   avg(AIR_TIME)|\n",
      "+----------------+\n",
      "|196.135770609319|\n",
      "+----------------+\n",
      "\n",
      "+------------------+\n",
      "| sum(duration_hrs)|\n",
      "+------------------+\n",
      "|2359307.1666666847|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_num.filter(flights_num.CARRIER == \"DL\")\\\n",
    "           .filter(flights_num.ORIGIN == \"SEA\")\\\n",
    "           .groupBy()\\\n",
    "           .avg(\"AIR_TIME\").show()\n",
    "    \n",
    "    \n",
    "flights_num.withColumn(\"duration_hrs\", flights_num.AIR_TIME/60)\\\n",
    "           .groupBy()\\\n",
    "           .sum(\"duration_hrs\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping & Aggregating I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|FL_NUM|count|\n",
      "+------+-----+\n",
      "|   296|  496|\n",
      "|  1090|  744|\n",
      "|  1159|  970|\n",
      "|  1572|  758|\n",
      "|  1512|  478|\n",
      "|   829|  295|\n",
      "|  1436|  598|\n",
      "|  2069|  402|\n",
      "|  2088|  232|\n",
      "|  2136|  405|\n",
      "|  2294|  187|\n",
      "|   467|  278|\n",
      "|   691|  325|\n",
      "|  2162|  208|\n",
      "|   675|  281|\n",
      "|   125|  360|\n",
      "|  1372|  395|\n",
      "|  2275|  385|\n",
      "|  2464|  595|\n",
      "|   800|   49|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_plane = flights.groupBy(\"FL_NUM\")\n",
    "by_plane.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|ORIGIN|     avg(AIR_TIME)|\n",
      "+------+------------------+\n",
      "|   MSY|102.44792872020366|\n",
      "|   GEG|110.44017094017094|\n",
      "|   SNA|160.89759815906802|\n",
      "|   GRB| 51.48905109489051|\n",
      "|   GRR| 69.30209481808159|\n",
      "|   GSO|53.421578421578424|\n",
      "|   PVD|122.09344660194175|\n",
      "|   MYR|54.166666666666664|\n",
      "|   OAK| 83.61252115059222|\n",
      "|   MSN|62.285419532324624|\n",
      "|   FAR| 38.67024128686327|\n",
      "|   DCA|127.56614850890654|\n",
      "|   LEX|58.769673704414586|\n",
      "|   ORF|101.29183673469387|\n",
      "|   EVV| 54.18181818181818|\n",
      "|   CRW| 58.55080213903744|\n",
      "|   SAV|40.853939045428405|\n",
      "|   TRI|  42.3859649122807|\n",
      "|   CMH| 108.8392026578073|\n",
      "|   CAK| 79.54024390243903|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "by_origin = flights_num.groupBy(\"ORIGIN\")\n",
    "by_origin.avg(\"AIR_TIME\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping & Aggregating II\n",
    "\n",
    "* In addition the the `GroupedData` methods, there is also `.agg()`, which lets us pass an aggregate column expression that uses any of the aggregate functions from the `pyspark.sql.functions` submodule\n",
    "\n",
    "* This submodule contains many useful functions for computing things like standard deviation, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_num = flights.select(flights.DISTANCE.cast(\"float\"), \n",
    "                             flights.ORIGIN, \n",
    "                             flights.CARRIER,\n",
    "                             flights.MONTH,\n",
    "                             flights.DEST,\n",
    "                             flights.AIR_TIME.cast(\"float\"),\n",
    "                             flights.DEP_DELAY.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+\n",
      "|MONTH|DEST|     avg(DEP_DELAY)|\n",
      "+-----+----+-------------------+\n",
      "|    1| BNA|  7.274881516587678|\n",
      "|    1| PNS|  5.964102564102564|\n",
      "|    2| CAE|   8.24731182795699|\n",
      "|    2| LIT|  8.904109589041095|\n",
      "|    3| BDL|  9.492374727668846|\n",
      "|    6| DLH|-0.2564102564102564|\n",
      "|    1| PDX|  12.58909090909091|\n",
      "|    1| ABE|-0.6086956521739131|\n",
      "|    5| PIT| 12.168758716875871|\n",
      "|    7| BDL|  6.613526570048309|\n",
      "|    1| AVP|               16.5|\n",
      "|    2| TUS|  9.970297029702971|\n",
      "|    5| LAX| 10.368102288021534|\n",
      "|    7| STT| 17.967441860465115|\n",
      "|    5| EWR| 14.490764063811923|\n",
      "|    1| MFE| 10.473333333333333|\n",
      "|    1| MDT|   9.61344537815126|\n",
      "|    3| PBI| 10.913649025069638|\n",
      "|    4| JFK| 15.184190902311707|\n",
      "|    4| MSY|  9.760371959942775|\n",
      "+-----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+----+----------------------+\n",
      "|MONTH|DEST|stddev_samp(DEP_DELAY)|\n",
      "+-----+----+----------------------+\n",
      "|    1| BNA|    27.874270456937698|\n",
      "|    1| PNS|     38.02893287563469|\n",
      "|    2| CAE|      24.9676555610839|\n",
      "|    2| LIT|     26.62752756780311|\n",
      "|    3| BDL|    28.537829153948827|\n",
      "|    6| DLH|    13.066062807936625|\n",
      "|    1| PDX|    27.097309852375698|\n",
      "|    1| ABE|     6.379933958298704|\n",
      "|    5| PIT|     38.86843192454137|\n",
      "|    7| BDL|    21.860008276927317|\n",
      "|    1| AVP|     31.73326330524486|\n",
      "|    2| TUS|    32.882226759549724|\n",
      "|    5| LAX|     38.97994389959673|\n",
      "|    7| STT|    45.996889984923484|\n",
      "|    5| EWR|    44.675146276874436|\n",
      "|    1| MFE|     27.13097654238005|\n",
      "|    1| MDT|     34.30523734381109|\n",
      "|    3| PBI|    31.130451171014265|\n",
      "|    4| JFK|     61.25457089174756|\n",
      "|    4| MSY|    31.162776335298226|\n",
      "+-----+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "by_month_dest = flights_num.groupBy(\"MONTH\", \"DEST\")\n",
    "by_month_dest.avg(\"DEP_DELAY\").show()\n",
    "by_month_dest.agg(F.stddev(\"DEP_DELAY\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No airports table, but this is how we'd do it\n",
    "# flights_with_aiports = flights.join(airports, on=\"dest\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Machine Learning Pipelines\n",
    "\n",
    "* At the core of the `pyspark.ml` module are the `Transformer` and `Estimator` classes\n",
    "* `Transformer` classes have a `.transform()` method that takes a DataFrame and returns a new DataFrame (usually the original with a new column appended)\n",
    "    * For example, we might use the class `Bucketizer` to create discrete bins from a continuous feature, or the class `PCA` to reduce the dimensionality of our dataset using principal component analysis\n",
    "* `Estimator` classes all implement a `.fit()` method\n",
    "* These methods take a DataFrame and return a model object\n",
    "    * For example, we can use `StringIndexerModel` for including categorical data saved as strings in our models, or a `RandomForestModel` that uses the RF algorithm for classification or regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+-----+------+-------------+-----------+----+\n",
      "|tailnum|type|manufacturer|issue_date|model|status|aircraft_type|engine_type|year|\n",
      "+-------+----+------------+----------+-----+------+-------------+-----------+----+\n",
      "| N050AA|null|        null|      null| null|  null|         null|       null|null|\n",
      "+-------+----+------------+----------+-----+------+-------------+-----------+----+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|_c0|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|  0|2008|    1|         3|        4| 2003.0|      1955| 2211.0|      2225|           WN|      335| N712SW|            128.0|         150.0|  116.0|   -14.0|     8.0|   IAD| TPA|     810|   4.0|    8.0|        0|               N|       0|        null|        null|    null|         null|             null|\n",
      "+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planes = spark.read.csv(\"plane-data.csv\", header=True)\n",
    "planes.show(1)\n",
    "\n",
    "flights = spark.read.csv(\"DelayedFlights.csv\", header=True)\n",
    "flights.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = planes.withColumnRenamed(\"year\", \"plane_year\")\n",
    "planes = planes.withColumnRenamed(\"tailnum\", \"TailNum\")\n",
    "model_data = flights.join(planes, on=\"TailNum\", how=\"leftouter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+-----------+------------+----------+-------+------+--------------------+-----------+----------+\n",
      "|TailNum|_c0|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|       type|manufacturer|issue_date|  model|status|       aircraft_type|engine_type|plane_year|\n",
      "+-------+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+-----------+------------+----------+-------+------+--------------------+-----------+----------+\n",
      "| N712SW|  0|2008|    1|         3|        4| 2003.0|      1955| 2211.0|      2225|           WN|      335|            128.0|         150.0|  116.0|   -14.0|     8.0|   IAD| TPA|     810|   4.0|    8.0|        0|               N|       0|        null|        null|    null|         null|             null|Corporation|      BOEING|07/08/1998|737-7H4| Valid|Fixed Wing Multi-...|  Turbo-Fan|      1998|\n",
      "| N772SW|  1|2008|    1|         3|        4|  754.0|       735| 1002.0|      1000|           WN|     3231|            128.0|         145.0|  113.0|     2.0|    19.0|   IAD| TPA|     810|   5.0|   10.0|        0|               N|       0|        null|        null|    null|         null|             null|Corporation|      BOEING|08/07/2000|737-7H4| Valid|Fixed Wing Multi-...|  Turbo-Fan|      2000|\n",
      "+-------+---+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+-----------+------------+----------+-------+------+--------------------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting String to Int/Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data\\\n",
    "            .withColumn(\"ArrDelay\", \\\n",
    "                        model_data.ArrDelay.cast(\"integer\"))\n",
    "model_data = model_data\\\n",
    "            .withColumn(\"AirTime\", \\\n",
    "                        model_data.ArrTime.cast(\"integer\"))\n",
    "model_data = model_data\\\n",
    "            .withColumn(\"Month\", \\\n",
    "                        model_data.Month.cast(\"integer\"))\n",
    "model_data = model_data\\\n",
    "            .withColumn(\"plane_year\", \\\n",
    "                        model_data.plane_year.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Age of a Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data\\\n",
    "            .withColumn(\"plane_age\", \n",
    "                        model_data.Year - model_data.plane_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Label (Is the plane late?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data\\\n",
    "            .withColumn(\"is_late\", model_data.ArrDelay > 0)\n",
    "    \n",
    "model_data = model_data\\\n",
    "            .withColumn(\"label\", \\\n",
    "                       model_data.is_late.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_nulls = \"\"\"ArrDelay is not null and DepDelay is not null\n",
    "                  and AirTime is not null and plane_year is not null\n",
    "               \"\"\"\n",
    "model_data = model_data.filter(remove_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "carr_indexer = StringIndexer(inputCol=\"UniqueCarrier\", \n",
    "                             outputCol=\"carrier_index\")\n",
    "\n",
    "carr_encoder = OneHotEncoder(inputCol=\"carrier_index\",\n",
    "                             outputCol=\"carrier_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_indexer = StringIndexer(inputCol=\"Dest\",\n",
    "                            outputCol=\"dest_index\")\n",
    "\n",
    "dest_encoder = OneHotEncoder(inputCol=\"dest_index\",\n",
    "                            outputCol=\"dest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble a Vector\n",
    "\n",
    "* Combine all of the columns containing our features into a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=[\"Month\", \"AirTime\",\n",
    "                                           \"carrier_fact\", \"dest_fact\",\n",
    "                                           \"plane_age\"],\n",
    "                                outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pipeline\n",
    "\n",
    "* `Pipeline` is a class in `pyspark.ml` which combines all the estimators and transformers we've already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_pipe = Pipeline(stages=[dest_indexer, dest_encoder,\n",
    "                                carr_indexer, carr_encoder,\n",
    "                                vec_assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "piped_data = flights_pipe.fit(model_data).transform(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = piped_data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "import numpy as np\n",
    "\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0., 0.1, 0.01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0, 1])\n",
    "\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=lr,\n",
    "                         estimatorParamMaps=grid,\n",
    "                         evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_4433bd993f0bdbe9c50b\n"
     ]
    }
   ],
   "source": [
    "models = cv.fit(training)\n",
    "best_lr = models.bestModel\n",
    "print(best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6103597602744483\n"
     ]
    }
   ],
   "source": [
    "test_results = best_lr.transform(test)\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
